{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiMarix1/NetworkIntelligence/blob/main/ANN_rawFile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-hLo5ufkCT1"
      },
      "source": [
        "## Text preprocessing worth the time: A comparative survey on the impact of common techniques on NLP model performances.\n",
        "- - -\n",
        "ANN ON FNS DS EXPERIMENTS NOTEBOOK\n",
        "- - -\n",
        "Artificial Neural Network on Fake News Spreaders Dataset.\n",
        "Code by M. Siino.\n",
        "\n",
        "From the paper: \"Text preprocessing worth the time: A comparative survey on the impact of common techniques on NLP model performances.\" by M.Siino et al.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IBqUcj4cx2G"
      },
      "source": [
        "## Importing modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQSunQ-ucjLX",
        "outputId": "17c19f1a-c68c-4638-cc3e-3a91d3499d3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import preprocessing\n",
        "from keras.models import Model\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import TextBlob\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "os.environ['TF_CUDNN_DETERMINISTIC']='true'\n",
        "os.environ['TF_DETERMINISTIC_OPS']='true'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QHd_fxmHCfa"
      },
      "source": [
        "## Importing DS and extract in current working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ocYMUXaY8r0_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "536896c0-c75f-4d21-9a19-2dfabda74f30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://github.com/marco-siino/fake_news_spreaders_detection/raw/main/dataset/pan20-author-profiling-training-2020-02-23.zip\n",
            "\u001b[1m3094459/3094459\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://github.com/marco-siino/fake_news_spreaders_detection/raw/main/dataset/pan20-author-profiling-test-2020-02-23.zip\n",
            "\u001b[1m2135236/2135236\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "./pan20-author-profiling-training-2020-02-23.zip\n",
            "./pan20-author-profiling-training-2020-02-23\n",
            ".config\t\t\t\t\tpan20-author-profiling-test-2020-02-23.zip\tsample_data\n",
            "__MACOSX\t\t\t\tpan20-author-profiling-training-2020-02-23\n",
            "pan20-author-profiling-test-2020-02-23\tpan20-author-profiling-training-2020-02-23.zip\n"
          ]
        }
      ],
      "source": [
        "# Url obtained starting from this: https://drive.google.com/file/d/19ZcqEv88euKB71HfAWjTGN3uCKp2qsfP/ and forcing export=download.\n",
        "urlTrainingSet = \"https://github.com/marco-siino/fake_news_spreaders_detection/raw/main/dataset/pan20-author-profiling-training-2020-02-23.zip\"\n",
        "urlTestSet=\"https://github.com/marco-siino/fake_news_spreaders_detection/raw/main/dataset/pan20-author-profiling-test-2020-02-23.zip\"\n",
        "\n",
        "training_set = tf.keras.utils.get_file(\"pan20-author-profiling-training-2020-02-23.zip\", urlTrainingSet,\n",
        "                                    extract=True, archive_format='zip',cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "test_set = tf.keras.utils.get_file(\"pan20-author-profiling-test-2020-02-23.zip\", urlTestSet,\n",
        "                                    extract=True, archive_format='zip',cache_dir='.',\n",
        "                                    cache_subdir='')\n",
        "\n",
        "training_set_dir = os.path.join(os.path.dirname(training_set), 'pan20-author-profiling-training-2020-02-23')\n",
        "test_set_dir = os.path.join(os.path.dirname(test_set), 'pan20-author-profiling-test-2020-02-23')\n",
        "\n",
        "print(training_set)\n",
        "print(training_set_dir)\n",
        "\n",
        "!ls -A\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTJi1fbT_Rup"
      },
      "source": [
        "## Build folders hierarchy to use Keras folders preprocessing function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FyafvXEMhEKw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f339d80-1cb8-4ab1-b8bd-ce31541b0151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".config\t\t\t\t\t    pan20-author-profiling-training-2020-02-23.zip\n",
            "__MACOSX\t\t\t\t    sample_data\n",
            "pan20-author-profiling-test-2020-02-23\t    test_dir_en\n",
            "pan20-author-profiling-test-2020-02-23.zip  train_dir_en\n",
            "pan20-author-profiling-training-2020-02-23\n"
          ]
        }
      ],
      "source": [
        "### Training Folders. ###\n",
        "\n",
        "# First level directory.\n",
        "if not os.path.exists('train_dir_en'):\n",
        "    os.makedirs('train_dir_en')\n",
        "\n",
        "# Class labels directory.\n",
        "if not os.path.exists('train_dir_en/0'):\n",
        "    os.makedirs('train_dir_en/0')\n",
        "if not os.path.exists('train_dir_en/1'):\n",
        "    os.makedirs('train_dir_en/1')\n",
        "\n",
        "# Make Py variables.\n",
        "train_dir='train_dir_'\n",
        "\n",
        "## Test Folders. ##\n",
        "# First level directory.\n",
        "if not os.path.exists('test_dir_en'):\n",
        "    os.makedirs('test_dir_en')\n",
        "\n",
        "# Class labels directory.\n",
        "if not os.path.exists('test_dir_en/0'):\n",
        "    os.makedirs('test_dir_en/0')\n",
        "if not os.path.exists('test_dir_en/1'):\n",
        "    os.makedirs('test_dir_en/1')\n",
        "\n",
        "# Make Py variables.\n",
        "test_dir='test_dir_'\n",
        "\n",
        "!ls -A"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set language and directory paths."
      ],
      "metadata": {
        "id": "-6c28Rdo8zGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set en and es ground truth file path for train_dir. We haven't a ground truth file for the test set.\n",
        "language='en'\n",
        "\n",
        "truth_file_training_dir_en=training_set_dir+'/'+language+'/'\n",
        "truth_file_training_path_en = truth_file_training_dir_en+'truth.txt'\n",
        "\n",
        "truth_file_test_dir=test_set_dir\n",
        "truth_file_test_path_en = truth_file_test_dir+'/'+language+'.txt'"
      ],
      "metadata": {
        "id": "ANr_dCj-8yvD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read truth.txt to organize training and test dataset folders."
      ],
      "metadata": {
        "id": "VU6PtNMZ864o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the file truth.txt with read only permit.\n",
        "f = open(truth_file_training_path_en, \"r\")\n",
        "# use readline() to read the first line\n",
        "line = f.readline()\n",
        "# use the read line to read further.\n",
        "# If the file is not empty keep reading one line\n",
        "# at a time, till the file is empty\n",
        "while line:\n",
        "    # Split line at :::\n",
        "    x = line.split(\":::\")\n",
        "    fNameXml = x[0]+'.xml'\n",
        "    fNameTxt = x[0]+'.txt'\n",
        "    # Second coord [0] gets just the first character (label) and not /n too.\n",
        "    label = x[1][0]\n",
        "\n",
        "    # Now move the file to the right folder.\n",
        "    if os.path.exists(truth_file_training_dir_en+fNameXml):\n",
        "      os.rename(truth_file_training_dir_en+fNameXml, './train_dir_'+language+'/'+label+'/'+fNameTxt )\n",
        "\n",
        "    # use readline() to read next line\n",
        "    line = f.readline()\n",
        "\n",
        "# Open the file truth.txt with read only permit.\n",
        "f = open(truth_file_test_path_en, \"r\")\n",
        "# use readline() to read the first line\n",
        "line = f.readline()\n",
        "# use the read line to read further.\n",
        "# If the file is not empty keep reading one line\n",
        "# at a time, till the file is empty\n",
        "while line:\n",
        "    # Split line at :::\n",
        "    x = line.split(\":::\")\n",
        "    fNameXml = x[0]+'.xml'\n",
        "    fNameTxt = x[0]+'.txt'\n",
        "    # Second coord [0] gets just the first character (label) and not /n too.\n",
        "    label = x[1][0]\n",
        "\n",
        "    # Now move the file to the right folder.\n",
        "    if os.path.exists(truth_file_test_dir+'/'+language+'/'+fNameXml):\n",
        "      os.rename(truth_file_test_dir+'/'+language+'/'+fNameXml, './test_dir_'+language+'/'+label+'/'+fNameTxt )\n",
        "\n",
        "    # use readline() to read next line\n",
        "    line = f.readline()"
      ],
      "metadata": {
        "id": "iW2UF_SH87TX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate full training set."
      ],
      "metadata": {
        "id": "4vskGJeX9GqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate full randomized training set.\n",
        "batch_size=1\n",
        "\n",
        "en_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    train_dir+language,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        "    )\n",
        "\n",
        "en_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
        "    test_dir+language,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        "    )\n",
        "\n",
        "train_ds=en_train_ds.shuffle(300,seed=13, reshuffle_each_iteration=False)\n",
        "test_ds=en_test_ds.shuffle(200,seed=13, reshuffle_each_iteration=False)\n",
        "\n",
        "train_ds_size=len(train_ds)\n",
        "test_ds_size=len(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOqIc_Sm9HGX",
        "outputId": "e2aab3c3-1a47-4125-fb7c-7586f0b1a3f7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 300 files belonging to 2 classes.\n",
            "Found 200 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITouXXtQ8WzV"
      },
      "source": [
        "## Functions to pre-process source text. (A detailed discussion on our paper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bDPIqAgXYWim"
      },
      "outputs": [],
      "source": [
        "# Do-Nothing preprocessing function.\n",
        "def DON(input_data):\n",
        "  tag_open_CDATA_removed = tf.strings.regex_replace(input_data, '<\\!\\[CDATA\\[', ' ')\n",
        "  tag_closed_CDATA_removed = tf.strings.regex_replace(tag_open_CDATA_removed,'\\]{1,}>', ' ')\n",
        "  tag_author_lang_en_removed = tf.strings.regex_replace(tag_closed_CDATA_removed,'<author lang=\"en\">', ' ')\n",
        "  tag_closed_author_removed = tf.strings.regex_replace(tag_author_lang_en_removed,'</author>', ' ')\n",
        "  tag_open_documents_removed = tf.strings.regex_replace(tag_closed_author_removed,'<documents>\\n(\\t){0,2}', '')\n",
        "  output_data = tf.strings.regex_replace(tag_open_documents_removed,'</documents>\\n(\\t){0,2}', ' ')\n",
        "  return output_data\n",
        "\n",
        "# Lowercasing preprocessing function.\n",
        "def LOW(input_data):\n",
        "  return tf.strings.lower(DON(input_data))\n",
        "\n",
        "# Removing Stop Words function.\n",
        "def RSW(input_data):\n",
        "  output_data = DON(input_data)\n",
        "\n",
        "  #print(\"\\n\\nInput data è il seguente tensore:\")\n",
        "  #print(output_data)\n",
        "\n",
        "  #print(\"Lo converto in stringa e diventa:\")\n",
        "  # Il seguente try per l'adattamento del ts. Nell'except caso della simulazione vera e propria.\n",
        "  try:\n",
        "    input_string=output_data[0]\n",
        "\n",
        "  # # # # # # # Questo è il caso della chiamata a funzione per la simulazione vera e propria.\n",
        "  except:\n",
        "    #print(\"\\n\\n****CASO DELLA SIMULAZIONE VERA E PROPRIA****\\n\\n\")\n",
        "    #print(\"\\nQuesto è il contenuto di output data in caso di simulazione\")\n",
        "    #print(output_data)\n",
        "    input_string=output_data\n",
        "\n",
        "    try:\n",
        "      input_string = input_string.numpy()\n",
        "\n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      #print(\"\\nEstraendo il contenuto del tensore risulta:\")\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [word for word in blob if word not in stopwords.words('english')]\n",
        "    #print(\"tolte le stopword inglesi diventa:\")\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)\n",
        "\n",
        "    output_tensor=tf.constant(output_string)\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "   # # # # # # # Questo è il caso dell'adattamento del TS.\n",
        "  else:\n",
        "\n",
        "    try:\n",
        "\n",
        "      # input_string = input_string.numpy() [0]\n",
        "      input_string = input_string.numpy()\n",
        "\n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [word for word in blob if word not in stopwords.words('english')]\n",
        "    #print(\"Tolte le stopword inglesi diventa:\")\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)\n",
        "\n",
        "    output_tensor=tf.constant([[output_string]])\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "  return output_data\n",
        "\n",
        "# Porter Stemmer preprocessing function.\n",
        "def STM(input_data):\n",
        "  output_data = DON(input_data)\n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  #print(\"\\n\\nInput data è il seguente tensore:\")\n",
        "  #print(output_data)\n",
        "\n",
        "  #print(\"Lo converto in stringa e diventa:\")\n",
        "  # Il seguente try per l'adattamento del ts. Nell'except caso della simulazione vera e propria.\n",
        "  try:\n",
        "    input_string=output_data[0]\n",
        "\n",
        "  # # # # # # # Questo è il caso della chiamata a funzione per la simulazione vera e propria.\n",
        "  except:\n",
        "    #print(\"\\n\\n****CASO DELLA SIMULAZIONE VERA E PROPRIA****\\n\\n\")\n",
        "    #print(\"\\nQuesto è il contenuto di output data in caso di simulazione\")\n",
        "    #print(output_data)\n",
        "    input_string=output_data\n",
        "\n",
        "    try:\n",
        "      input_string = input_string.numpy()\n",
        "\n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      #print(\"\\nEstraendo il contenuto del tensore risulta:\")\n",
        "      #print(input_string)\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [stemmer.stem(word) for word in blob]\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "    #print(output_string)\n",
        "\n",
        "    output_tensor=tf.constant(output_string)\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "   # # # # # # # Questo è il caso dell'adattamento del TS.\n",
        "  else:\n",
        "\n",
        "    try:\n",
        "      #input_string = input_string.numpy()[0]\n",
        "      input_string = input_string.numpy()\n",
        "      #print(input_string)\n",
        "\n",
        "    except:\n",
        "      #print(\"This one is not a tensor!\")\n",
        "      return output_data\n",
        "\n",
        "    else:\n",
        "      input_string=(str(input_string))[2:-1]\n",
        "\n",
        "    #print(input_string)\n",
        "    blob = TextBlob(str(input_string)).words\n",
        "\n",
        "    outputlist = [stemmer.stem(word) for word in blob]\n",
        "\n",
        "    output_string = (' '.join(word for word in outputlist))\n",
        "\n",
        "    output_tensor=tf.constant([[output_string]])\n",
        "    #print(output_tensor)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "  return output_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgfUp_5TWED7"
      },
      "source": [
        "## Define the combined preprocessing functions. (The base functions are: DON, LOW, RSW and STM)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Mtlue6FuWIaB"
      },
      "outputs": [],
      "source": [
        "## SECTION WITH PAIRS OF PREPRO FUNCTIONS. APPLICATION ORDER MATTERS (...IN FOLLOWING SECTIONS TOO).\n",
        "#...5\n",
        "def LOW_RSW(input_data):\n",
        "  return RSW(LOW(input_data))\n",
        "\n",
        "# 6\n",
        "def LOW_STM(input_data):\n",
        "  return STM(LOW(input_data))\n",
        "\n",
        "# 7\n",
        "def RSW_LOW(input_data):\n",
        "  return LOW(RSW(input_data))\n",
        "\n",
        "# 8\n",
        "def RSW_STM(input_data):\n",
        "  return STM(RSW(input_data))\n",
        "\n",
        "# 9\n",
        "def STM_LOW(input_data):\n",
        "  return LOW(STM(input_data))\n",
        "\n",
        "# 10\n",
        "def STM_RSW(input_data):\n",
        "  return RSW(STM(input_data))\n",
        "\n",
        "# 11\n",
        "def LOW_STM_RSW(input_data):\n",
        "  return RSW(STM(LOW(input_data)))\n",
        "\n",
        "# 12\n",
        "def LOW_RSW_STM(input_data):\n",
        "  return STM(RSW(LOW(input_data)))\n",
        "\n",
        "# 13\n",
        "def STM_LOW_RSW(input_data):\n",
        "  return RSW(LOW(STM(input_data)))\n",
        "\n",
        "# 14\n",
        "def STM_RSW_LOW(input_data):\n",
        "  return LOW(RSW(STM(input_data)))\n",
        "\n",
        "# 15\n",
        "def RSW_LOW_STM(input_data):\n",
        "  return STM(LOW(RSW(input_data)))\n",
        "\n",
        "# 16\n",
        "def RSW_STM_LOW(input_data):\n",
        "  return LOW(STM(RSW(input_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjUnE9_DEyCw"
      },
      "source": [
        "## Get the length of the longest sample in training set. Then adapt text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ebRMQqa_mr48"
      },
      "outputs": [],
      "source": [
        "def preprocess_and_adapt_ts(preprocessing_function,training_set):\n",
        "  # Set a large sequence length to find the longest sample in the training set.\n",
        "  sequence_length = 3000\n",
        "  vectorize_layer = TextVectorization(\n",
        "      standardize=preprocessing_function,\n",
        "      output_mode='int',\n",
        "      output_sequence_length=sequence_length)\n",
        "\n",
        "  train_text = training_set.map(lambda x, y: x)\n",
        "  vectorize_layer.adapt(train_text)\n",
        "  #vectorize_layer.get_vocabulary()\n",
        "\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "  model.add(vectorize_layer)\n",
        "\n",
        "  longest_sample_length=1\n",
        "\n",
        "  for element in training_set:\n",
        "    authorDocument=element[0]\n",
        "    label=element[1]\n",
        "\n",
        "    #print(\"Sample considered is: \", authorDocument[0].numpy())\n",
        "    #print(\"Preprocessed: \", str(custom_standardization(authorDocument[0].numpy())))\n",
        "    #print(\"And has label: \", label[0].numpy())\n",
        "\n",
        "    out=model(authorDocument)\n",
        "    # Convert token list to numpy array.\n",
        "    token_list = out.numpy()[0]\n",
        "    token_list = np.trim_zeros(token_list,'b')\n",
        "    if longest_sample_length < len(token_list):\n",
        "      longest_sample_length = len(token_list)\n",
        "\n",
        "  print(\"Length of the longest sample is:\", longest_sample_length)\n",
        "\n",
        "  # After tokenization longest_sample_length covers all the document lenghts in our dataset.\n",
        "  sequence_length = longest_sample_length\n",
        "\n",
        "  vectorize_layer = TextVectorization(\n",
        "      standardize=preprocessing_function,\n",
        "      output_mode='int',\n",
        "      output_sequence_length=sequence_length)\n",
        "\n",
        "  # Finally adapt the vectorize layer.\n",
        "  train_text = training_set.map(lambda x, y: x)\n",
        "  vectorize_layer.adapt(train_text)\n",
        "  return vectorize_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8zDXrk9eEy5"
      },
      "source": [
        "## Define a dictionary with -> function_names:prepro_function_caller. And a dictionary to store model results.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "P8owxNMIeFsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d63642d4-51cb-4827-8cc9-e9b3c9e13055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DON\n"
          ]
        }
      ],
      "source": [
        "model_results = {}\n",
        "prepro_functions_dict_base = {\n",
        "    'DON':DON,\n",
        "    'LOW':LOW,\n",
        "    'RSW':RSW,\n",
        "    'STM':STM\n",
        "    }\n",
        "\n",
        "# 3 prepro functions = 15 combs...+1 for do_nothing\n",
        "\n",
        "prepro_functions_dict_comb = {\n",
        "    # 1. Do nothing\n",
        "    'DON': DON,\n",
        "    # 2. Lowercasing\n",
        "    # 'LOW':LOW,\n",
        "    # 3. Removing Stopwords\n",
        "    # 'RSW':RSW,\n",
        "    # # 4. Porter Stemming\n",
        "    # 'STM':STM,\n",
        "    # 5. LOW->RSW\n",
        "    # 'LOW_RSW':LOW_RSW,\n",
        "    # # 6. LOW->STM\n",
        "    # 'LOW_STM':LOW_STM,\n",
        "    # # 7. RSW->LOW\n",
        "    # 'RSW_LOW':RSW_LOW,\n",
        "    # # 8. RSW->STM\n",
        "    # 'RSW_STM':RSW_STM,\n",
        "    # # 9. STM->LOW\n",
        "    # 'STM_LOW':STM_LOW,\n",
        "    # # 10. STM->RSW\n",
        "    # 'STM_RSW':STM_RSW,\n",
        "    # # 11. LOW->STM->RSW\n",
        "    # 'LOW_STM_RSW':LOW_STM_RSW,\n",
        "    # # 12. LOW->RSW->STM\n",
        "    # 'LOW_RSW_STM':LOW_RSW_STM,\n",
        "    # # 13. STM->LOW->RSW\n",
        "    # 'STM_LOW_RSW':STM_LOW_RSW,\n",
        "    # # 14. STM->RSW->LOW\n",
        "    # 'STM_RSW_LOW':STM_RSW_LOW,\n",
        "    # # 15. RSW->LOW->STM\n",
        "    # 'RSW_LOW_STM':RSW_LOW_STM,\n",
        "    # # 16. RSW->STM->LOW\n",
        "    # 'RSW_STM_LOW':RSW_STM_LOW\n",
        "}\n",
        "\n",
        "for key in prepro_functions_dict_comb:\n",
        "  print(key)\n",
        "  model_results[key]=[]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some training hyperparameters..."
      ],
      "metadata": {
        "id": "RF7pJkGJ3bHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word embedding dimensions.\n",
        "embedding_dim = 100\n",
        "\n",
        "num_runs = 3\n",
        "# No need to go over the 20th epoch...Overfitting begins.\n",
        "num_epochs_per_run = 10\n",
        "\n",
        "opt = tf.keras.optimizers.RMSprop()"
      ],
      "metadata": {
        "id": "ocHligxS3mVn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDEi7MAo4qsQ"
      },
      "source": [
        "## Models definition and evaluation.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm1fdCpETWL-",
        "outputId": "18aeb2bb-37e8-4bc0-cb83-fe387423b5e0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "* * * * EVALUATION USING DON AS PREPROCESSING FUNCTION * * * *\n",
            "Length of the longest sample is: 2415\n",
            "Vocabulary size is: 68043\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - binary_accuracy: 0.5099 - loss: 0.9280 - val_binary_accuracy: 0.5000 - val_loss: 0.6932\n",
            "Run:  1 / Accuracy at epoch  0  is:  0.5 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - binary_accuracy: 0.4845 - loss: 0.7023 - val_binary_accuracy: 0.5000 - val_loss: 0.6942\n",
            "Run:  1 / Accuracy at epoch  1  is:  0.5 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - binary_accuracy: 0.4333 - loss: 0.6999 - val_binary_accuracy: 0.5000 - val_loss: 0.6931\n",
            "Run:  1 / Accuracy at epoch  2  is:  0.5 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - binary_accuracy: 0.4252 - loss: 0.7005 - val_binary_accuracy: 0.5000 - val_loss: 0.6933\n",
            "Run:  1 / Accuracy at epoch  3  is:  0.5 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - binary_accuracy: 0.4941 - loss: 0.6997 - val_binary_accuracy: 0.5000 - val_loss: 0.7527\n",
            "Run:  1 / Accuracy at epoch  4  is:  0.5 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - binary_accuracy: 0.5022 - loss: 0.7582 - val_binary_accuracy: 0.5900 - val_loss: 0.8155\n",
            "Run:  1 / Accuracy at epoch  5  is:  0.5899999737739563 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - binary_accuracy: 0.5774 - loss: 0.7076 - val_binary_accuracy: 0.5500 - val_loss: 0.7456\n",
            "Run:  1 / Accuracy at epoch  6  is:  0.550000011920929 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - binary_accuracy: 0.5765 - loss: 0.7302 - val_binary_accuracy: 0.5800 - val_loss: 0.8105\n",
            "Run:  1 / Accuracy at epoch  7  is:  0.5799999833106995 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - binary_accuracy: 0.6422 - loss: 0.7002 - val_binary_accuracy: 0.5950 - val_loss: 0.8230\n",
            "Run:  1 / Accuracy at epoch  8  is:  0.5950000286102295 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - binary_accuracy: 0.7057 - loss: 0.5973 - val_binary_accuracy: 0.5650 - val_loss: 0.9362\n",
            "Run:  1 / Accuracy at epoch  9  is:  0.5649999976158142 \n",
            "\n",
            "Accuracies over epochs: [0.5, 0.5, 0.5, 0.5, 0.5, 0.5899999737739563, 0.550000011920929, 0.5799999833106995, 0.5950000286102295, 0.5649999976158142] \n",
            "\n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - binary_accuracy: 0.4772 - loss: 0.8870 - val_binary_accuracy: 0.5000 - val_loss: 1.0371\n",
            "Run:  2 / Accuracy at epoch  0  is:  0.5 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - binary_accuracy: 0.4996 - loss: 0.7690 - val_binary_accuracy: 0.5000 - val_loss: 0.9817\n",
            "Run:  2 / Accuracy at epoch  1  is:  0.5 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - binary_accuracy: 0.4949 - loss: 0.7544 - val_binary_accuracy: 0.5500 - val_loss: 0.7442\n",
            "Run:  2 / Accuracy at epoch  2  is:  0.550000011920929 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - binary_accuracy: 0.6128 - loss: 0.7232 - val_binary_accuracy: 0.6150 - val_loss: 0.7663\n",
            "Run:  2 / Accuracy at epoch  3  is:  0.6150000095367432 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - binary_accuracy: 0.6575 - loss: 0.6905 - val_binary_accuracy: 0.5700 - val_loss: 0.8769\n",
            "Run:  2 / Accuracy at epoch  4  is:  0.5699999928474426 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - binary_accuracy: 0.7970 - loss: 0.4986 - val_binary_accuracy: 0.5500 - val_loss: 1.5422\n",
            "Run:  2 / Accuracy at epoch  5  is:  0.550000011920929 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - binary_accuracy: 0.9213 - loss: 0.2587 - val_binary_accuracy: 0.5750 - val_loss: 2.1959\n",
            "Run:  2 / Accuracy at epoch  6  is:  0.574999988079071 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - binary_accuracy: 0.9536 - loss: 0.1601 - val_binary_accuracy: 0.5850 - val_loss: 3.3384\n",
            "Run:  2 / Accuracy at epoch  7  is:  0.5849999785423279 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - binary_accuracy: 0.9668 - loss: 0.0837 - val_binary_accuracy: 0.7200 - val_loss: 1.4443\n",
            "Run:  2 / Accuracy at epoch  8  is:  0.7200000286102295 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - binary_accuracy: 0.9854 - loss: 0.0633 - val_binary_accuracy: 0.6050 - val_loss: 3.2181\n",
            "Run:  2 / Accuracy at epoch  9  is:  0.6050000190734863 \n",
            "\n",
            "Accuracies over epochs: [0.5, 0.5, 0.550000011920929, 0.6150000095367432, 0.5699999928474426, 0.550000011920929, 0.574999988079071, 0.5849999785423279, 0.7200000286102295, 0.6050000190734863] \n",
            "\n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 19ms/step - binary_accuracy: 0.4752 - loss: 0.9622 - val_binary_accuracy: 0.5000 - val_loss: 1.0459\n",
            "Run:  3 / Accuracy at epoch  0  is:  0.5 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - binary_accuracy: 0.4522 - loss: 0.7843 - val_binary_accuracy: 0.5900 - val_loss: 0.7160\n",
            "Run:  3 / Accuracy at epoch  1  is:  0.5899999737739563 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - binary_accuracy: 0.4837 - loss: 0.7126 - val_binary_accuracy: 0.5300 - val_loss: 0.8035\n",
            "Run:  3 / Accuracy at epoch  2  is:  0.5299999713897705 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - binary_accuracy: 0.5726 - loss: 0.7699 - val_binary_accuracy: 0.5950 - val_loss: 0.7541\n",
            "Run:  3 / Accuracy at epoch  3  is:  0.5950000286102295 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - binary_accuracy: 0.5203 - loss: 0.7125 - val_binary_accuracy: 0.5800 - val_loss: 0.7600\n",
            "Run:  3 / Accuracy at epoch  4  is:  0.5799999833106995 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 18ms/step - binary_accuracy: 0.6588 - loss: 0.7061 - val_binary_accuracy: 0.5800 - val_loss: 0.7749\n",
            "Run:  3 / Accuracy at epoch  5  is:  0.5799999833106995 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 20ms/step - binary_accuracy: 0.7795 - loss: 0.5489 - val_binary_accuracy: 0.5200 - val_loss: 1.0527\n",
            "Run:  3 / Accuracy at epoch  6  is:  0.5199999809265137 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - binary_accuracy: 0.8998 - loss: 0.3244 - val_binary_accuracy: 0.6650 - val_loss: 0.9726\n",
            "Run:  3 / Accuracy at epoch  7  is:  0.6650000214576721 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 18ms/step - binary_accuracy: 0.9532 - loss: 0.1023 - val_binary_accuracy: 0.6800 - val_loss: 1.2758\n",
            "Run:  3 / Accuracy at epoch  8  is:  0.6800000071525574 \n",
            "\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 19ms/step - binary_accuracy: 0.9777 - loss: 0.0738 - val_binary_accuracy: 0.6900 - val_loss: 1.6565\n",
            "Run:  3 / Accuracy at epoch  9  is:  0.6899999976158142 \n",
            "\n",
            "Accuracies over epochs: [0.5, 0.5899999737739563, 0.5299999713897705, 0.5950000286102295, 0.5799999833106995, 0.5799999833106995, 0.5199999809265137, 0.6650000214576721, 0.6800000071525574, 0.6899999976158142] \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tf.random.set_seed(13)\n",
        "\n",
        "# Reset model_results list.\n",
        "for key in prepro_functions_dict_comb:\n",
        "  model_results[key]=[]\n",
        "\n",
        "for key in prepro_functions_dict_comb:\n",
        "  runs_accuracy = []\n",
        "\n",
        "  print(\"\\n\\n* * * * EVALUATION USING\", key, \"AS PREPROCESSING FUNCTION * * * *\")\n",
        "\n",
        "  # Preprocess training set to build a dictionary.\n",
        "  vectorize_layer = preprocess_and_adapt_ts(prepro_functions_dict_comb[key],train_ds)\n",
        "\n",
        "  max_features=len(vectorize_layer.get_vocabulary()) + 1\n",
        "  print(\"Vocabulary size is:\", max_features)\n",
        "\n",
        "  for run in range(1,(num_runs+1)):\n",
        "    epochs_accuracy=[]\n",
        "    opt = tf.keras.optimizers.RMSprop(\n",
        "        learning_rate=0.005\n",
        "        )\n",
        "    model = tf.keras.Sequential([\n",
        "                                    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
        "                                    vectorize_layer,\n",
        "                                    layers.Embedding(max_features + 1, embedding_dim),\n",
        "                                    layers.Dropout(0.8),\n",
        "                                    layers.Dense(1024,activation='relu'),\n",
        "                                    layers.Dropout(0.7),\n",
        "                                    # layers.Dense(max_features//32,activation='relu'),\n",
        "                                    # layers.Dropout(0.4),\n",
        "                                    layers.Dense(512,activation='relu'),\n",
        "                                    layers.Dropout(0.6),\n",
        "                                    layers.Dense(256,activation='relu'),\n",
        "                                    layers.Dropout(0.45),\n",
        "\n",
        "                                    layers.GlobalAveragePooling1D(),\n",
        "                                    layers.Dropout(0.3),\n",
        "                                    layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(loss=losses.BinaryCrossentropy(from_logits=True), optimizer=opt, metrics=[tf.metrics.BinaryAccuracy(threshold=0.0)])\n",
        "\n",
        "    for epoch in range (0,num_epochs_per_run):\n",
        "        history = model.fit(\n",
        "          train_ds,\n",
        "          validation_data = test_ds,\n",
        "          epochs=1,\n",
        "          shuffle=False,\n",
        "          # Comment the following line to do not save and download the model.\n",
        "          #callbacks=[callbacks]\n",
        "          )\n",
        "        accuracy = history.history['val_binary_accuracy']\n",
        "        print(\"Run: \",run,\"/ Accuracy at epoch \",epoch,\" is: \", accuracy[0],\"\\n\")\n",
        "        epochs_accuracy.append(accuracy[0])\n",
        "\n",
        "    print(\"Accuracies over epochs:\",epochs_accuracy,\"\\n\\n\")\n",
        "    runs_accuracy.append(max(epochs_accuracy))\n",
        "\n",
        "  # runs_accuracy.sort()\n",
        "  # print(\"\\n\\n Over all runs maximum accuracies are:\", runs_accuracy)\n",
        "  # print(\"The median is:\",runs_accuracy[2],\"\\n\\n\\n\")\n",
        "\n",
        "  # if (runs_accuracy[2]-runs_accuracy[0])>(runs_accuracy[4]-runs_accuracy[2]):\n",
        "  #   max_range_from_median = runs_accuracy[2]-runs_accuracy[0]\n",
        "  # else:\n",
        "  #   max_range_from_median = runs_accuracy[4]-runs_accuracy[2]\n",
        "  # final_result = str(runs_accuracy[2])+\" +/- \"+ str(max_range_from_median)\n",
        "  # model_results[key].append(final_result)\n",
        "  # print(\"ANN Accuracy Score on Test set -> \",model_results[key])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uMKAeuegNk1"
      },
      "source": [
        "## Now show compact results in a table."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_features//32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjKIxLtHyorC",
        "outputId": "6fac06d3-f73c-414d-cc68-299a318eed18"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runs_accuracy.sort()\n",
        "print(\"\\n\\n Over all runs maximum accuracies are:\", runs_accuracy)\n",
        "print(\"The median is:\",runs_accuracy[2],\"\\n\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6DzY_XfwVea",
        "outputId": "df5df208-403c-47ac-c5a9-897707f48697"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " Over all runs maximum accuracies are: [0.5950000286102295, 0.6899999976158142, 0.7200000286102295]\n",
            "The median is: 0.7200000286102295 \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_results[key]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfVF31OiyM9e",
        "outputId": "81da329a-b214-4ab3-941d-037a18a62e2e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AJ79RMougNRA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "dd58dd68-ad5a-4bb2-f492-082a60a8ffa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " PREPRO FUNCTION    |  Test Accuracy   |\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-af0db470ae32>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprepro_func\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprepro_functions_dict_comb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#print(prepro_func,\"\\t\\t\\t\",format(round(model_results[prepro_func][0],4),'.4f'),\"\\t\\t\",end='')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprepro_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;31m# result = format(round(model_results[prepro_func][0],4),'.4f')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{prepro_func:27}{ result :12}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "print(\" PREPRO FUNCTION    |  Test Accuracy   |\",end = '')\n",
        "\n",
        "print(\"\\n\")\n",
        "for prepro_func in prepro_functions_dict_comb:\n",
        "  #print(prepro_func,\"\\t\\t\\t\",format(round(model_results[prepro_func][0],4),'.4f'),\"\\t\\t\",end='')\n",
        "  result = model_results[prepro_func][0]\n",
        "  # result = format(round(model_results[prepro_func][0],4),'.4f')\n",
        "  print(f'{prepro_func:27}{ result :12}')\n",
        "  print(\"\\n\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6IBqUcj4cx2G",
        "0QHd_fxmHCfa",
        "uTJi1fbT_Rup",
        "-6c28Rdo8zGn",
        "VU6PtNMZ864o",
        "4vskGJeX9GqJ",
        "RgfUp_5TWED7",
        "TjUnE9_DEyCw"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}